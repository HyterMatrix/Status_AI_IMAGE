{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入数据\n",
    "X = np.array([[1,0,0],\n",
    "              [1,0,1],\n",
    "              [1,1,0],\n",
    "              [1,1,1]])\n",
    "# 标签\n",
    "Y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "# 3-10-1\n",
    "# 取值范围-1到1\n",
    "V = np.random.random([3,10]) * 2 - 1\n",
    "W = np.random.random([10,1]) * 2 - 1\n",
    "# 学习率\n",
    "lr = 0.11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# 权值调整函数\n",
    "def update():\n",
    "    global V,W\n",
    "    \n",
    "    # 求每一层的输出\n",
    "    L1 = sigmoid(np.dot(X,V))\n",
    "    L2 = sigmoid(np.dot(L1,W))\n",
    "    \n",
    "    # 求每一层的学习信号\n",
    "    L2_delta = (Y-L2)*dsigmoid(L2)\n",
    "    L1_delta = L2_delta.dot(W.T)*dsigmoid(L1)\n",
    "    \n",
    "    # 求每一层权值的变化\n",
    "    delta_W = lr*L1.T.dot(L2_delta)\n",
    "    delta_V = lr*X.T.dot(L1_delta)\n",
    "    \n",
    "    # 改变权值\n",
    "    W = W + delta_W\n",
    "    V = V + delta_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.137418950209\n",
      "loss: 0.121260951473\n",
      "loss: 0.107101245696\n",
      "loss: 0.0764415467291\n",
      "loss: 0.0392228359566\n",
      "loss: 0.0182243059329\n",
      "loss: 0.00976711469814\n",
      "loss: 0.00607963996417\n",
      "loss: 0.00420982959612\n",
      "loss: 0.00313548364604\n",
      "loss: 0.00245787480365\n",
      "loss: 0.00199976699732\n",
      "loss: 0.00167327123226\n",
      "loss: 0.00143081635769\n",
      "loss: 0.00124478392045\n",
      "loss: 0.00109820229887\n",
      "loss: 0.000980143932956\n",
      "loss: 0.000883293647529\n",
      "loss: 0.000802590881753\n",
      "loss: 0.00073443511309\n",
      "loss: 0.000676201776146\n"
     ]
    }
   ],
   "source": [
    "for i in range(10001):\n",
    "    # 更新权值\n",
    "    update()\n",
    "    if i%500==0:\n",
    "        # 求每一层的输出\n",
    "        L1 = sigmoid(np.dot(X,V))\n",
    "        L2 = sigmoid(np.dot(L1,W))\n",
    "        loss = np.mean(np.square(Y-L2)/2)\n",
    "        print('loss:',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03326924]\n",
      " [ 0.96037161]\n",
      " [ 0.96654941]\n",
      " [ 0.0401674 ]]\n"
     ]
    }
   ],
   "source": [
    "L1 = sigmoid(np.dot(X,V))\n",
    "L2 = sigmoid(np.dot(L1,W))\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def judge(x):\n",
    "    if x>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i in map(judge,L2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
